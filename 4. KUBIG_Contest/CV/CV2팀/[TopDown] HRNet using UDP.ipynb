{"cells":[{"cell_type":"markdown","metadata":{"id":"XJ0YRnu_8OnE"},"source":["대회: https://dacon.io/competitions/official/235701/overview/description\n","\n","\n","참고: https://maihon.oopy.io/competition/dacon-keypoints\n","\n","\n","HRNet: https://github.com/leoxiaobin/deep-high-resolution-net.pytorch\n","\n","UDPpose: https://github.com/HuangJunJie2017/UDP-Pose"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7440,"status":"ok","timestamp":1677465343042,"user":{"displayName":"Claire Lim","userId":"01177983436534277283"},"user_tz":-540},"id":"nNJhoggI3Zp_","outputId":"799bcf94-8368-4ec4-dabe-f423f56793dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting git+https://github.com/albu/albumentations\n","  Cloning https://github.com/albu/albumentations to /tmp/pip-req-build-4ew97ul_\n","  Running command git clone --filter=blob:none --quiet https://github.com/albu/albumentations /tmp/pip-req-build-4ew97ul_\n","  Resolved https://github.com/albu/albumentations to commit cb372736a0b7da362e5c2e23f3cce4304ddac402\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.8/dist-packages (from albumentations==1.3.0) (1.22.4)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from albumentations==1.3.0) (1.7.3)\n","Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.8/dist-packages (from albumentations==1.3.0) (0.18.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.8/dist-packages (from albumentations==1.3.0) (6.0)\n","Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from albumentations==1.3.0) (0.0.4)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from albumentations==1.3.0) (4.6.0.66)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from qudida>=0.0.4->albumentations==1.3.0) (4.5.0)\n","Requirement already satisfied: opencv-python-headless>=4.0.1 in /usr/local/lib/python3.8/dist-packages (from qudida>=0.0.4->albumentations==1.3.0) (4.7.0.68)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.8/dist-packages (from qudida>=0.0.4->albumentations==1.3.0) (1.0.2)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (3.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (2023.2.3)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (1.4.1)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (2.9.0)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (3.5.3)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.16.1->albumentations==1.3.0) (7.1.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.3.0) (0.11.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.3.0) (23.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.3.0) (2.8.2)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.3.0) (4.38.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.3.0) (1.4.4)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.3.0) (3.0.9)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.0) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations==1.3.0) (3.1.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations==1.3.0) (1.15.0)\n","Building wheels for collected packages: albumentations\n","  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for albumentations: filename=albumentations-1.3.0-py3-none-any.whl size=125638 sha256=1875e167eb705dbd3c3cd9203280a5283be79a942658768033f50110f5a9dc02\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-yx7p4tc7/wheels/c5/ca/df/fae131e2d3a8174cd8668f10bf0591fa158f0824214d3017bc\n","Successfully built albumentations\n","Installing collected packages: albumentations\n","  Attempting uninstall: albumentations\n","    Found existing installation: albumentations 1.2.1\n","    Uninstalling albumentations-1.2.1:\n","      Successfully uninstalled albumentations-1.2.1\n","Successfully installed albumentations-1.3.0\n"]}],"source":["!pip install -U git+https://github.com/albu/albumentations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRW009UA3ZqG"},"outputs":[],"source":["import os\n","import cv2\n","import random\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import albumentations as A\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import StratifiedKFold, train_test_split\n","from typing import Tuple, List, Sequence, Callable\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as tfms\n","from torch.utils.data import DataLoader, Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2480,"status":"ok","timestamp":1677481970186,"user":{"displayName":"Claire Lim","userId":"01177983436534277283"},"user_tz":-540},"id":"tovu5uV53ZqH","outputId":"dad5407c-4873-43b4-ad85-e1fe7f10c659"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4lvDN_6J3ZqI"},"outputs":[],"source":["main_dir = '/content/drive/MyDrive/방학 CV분반 KUBIG CONTEST/임채명/kubigcontestdata'\n","train_img_path = os.path.join(main_dir, 'train_imgs')\n","test_img_path = os.path.join(main_dir, 'test_imgs')\n","meta_info_dir = os.path.join(main_dir, 'train_df.csv') # csv에서는 잘못된 데이터 삭제되어 있음 "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p-7C41P33ZqI"},"outputs":[],"source":["train_df = pd.read_csv(meta_info_dir)\n","train_df = train_df.reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{"id":"6E6tHoSRByNY"},"source":["- Seed 고정 https://dacon.io/codeshare/2363\n","- torch.use_deterministic_algorithms(True)\n","https://jh-bk.tistory.com/19"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ZpcEi6g3ZqJ"},"outputs":[],"source":["def seed_everything(seed=16):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    torch.use_deterministic_algorithms(True)\n","\n","    import imgaug\n","    imgaug.random.seed(seed)\n","\n","seed_everything(16)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d_E6pu6K3ZqJ"},"outputs":[],"source":["def show_image(cfg, image, keypoints, factor=None):\n","    if keypoints.shape[-1] == 3:\n","      keypoints = keypoints[:, :2].astype(np.int)\n","\n","    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n","    colors = cfg.joint_colors\n","\n","    if factor is not None:\n","      keypoints[:, 0] = keypoints[:, 0] * factor[0]\n","      keypoints[:, 1] = keypoints[:, 1] * factor[1]\n","\n","    x1, y1 = int(min(keypoints[:, 0])), int(min(keypoints[:, 1]))\n","    x2, y2 = int(max(keypoints[:, 0])), int(max(keypoints[:, 1]))\n","    cv2.rectangle(image, (x1, y1), (x2, y2), (255, 100, 91), thickness=3)\n","\n","    for i, keypoint in enumerate(keypoints):\n","        cv2.circle(\n","            image, \n","            tuple(keypoint), \n","            3, colors.get(i), thickness=2, lineType=cv2.FILLED)\n","\n","        cv2.putText(\n","            image, \n","            f'{i}: {cfg.joints_name[i]}', \n","            tuple(keypoint), \n","            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1)\n","\n","    for i, pair in enumerate(cfg.joint_pair):\n","        cv2.line(\n","            image, \n","            tuple(keypoints[pair[0]]), \n","            tuple(keypoints[pair[1]]),\n","            colors.get(pair[0]), 2, lineType=cv2.LINE_AA)\n","\n","    fig, ax = plt.subplots(dpi=200)\n","    ax.imshow(image)\n","    ax.axis('off')\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"HocMe43aCX1q"},"source":["- 테스트 환경 구축을 위한 Config 만들기\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"73W1u2dC3ZqK"},"outputs":[],"source":["from typing import List\n","\n","class SingleModelConfig:\n","  def __init__(self,\n","               \n","               input_size: List[int] = [512, 512],\n","               kpd: float = 4.0,\n","               epochs: int = 150,\n","               sigma: float = 3.0,\n","               num_joints: int = 24,\n","               batch_size: int = 16,\n","               random_seed: int = 2021,\n","               test_ratio: float = 0.15,\n","               learning_rate: float = 1e-3,\n","\n","               save_folder: str = 'result',\n","               main_dir: str = main_dir,\n","               loss_type: str = \"MSE\",\n","               target_type: str = \"gaussian\",\n","               post_processing: str = \"dark\",\n","\n","               debug: bool = False,\n","               shift: bool = False,\n","               startify: bool = False,\n","               init_training: bool = False,\n","               startify_with_dir: bool = True,\n","    ):\n","\n","    self.main_dir = main_dir\n","    self.save_folder = os.path.join(main_dir, save_folder)\n","    if not os.path.exists(self.save_folder) and self.save_folder != '':\n","      os.makedirs(self.save_folder, exist_ok=True)\n","\n","    self.epochs = epochs\n","    self.seed = random_seed\n","    self.lr = learning_rate\n","    self.loss_type = loss_type\n","    self.num_joints = num_joints\n","    self.batch_size = batch_size\n","    self.test_ratio = test_ratio\n","    self.init_training = init_training\n","\n","    self.kpd = kpd\n","    self.sigma = sigma\n","    self.shift = shift\n","    self.debug = debug\n","    self.startify = startify\n","    self.target_type = target_type\n","    self.image_size = np.array(input_size)\n","    self.output_size = self.image_size//4\n","    self.post_processing = post_processing\n","    self.startify_with_dir = startify_with_dir\n","\n","\n","    self.joints_name = {\n","          0: 'nose', 1: 'left_eye', 2: 'right_eye', 3: 'left_ear', 4: 'right_ear',\n","          5: 'left_shoulder', 6: 'right_shoulder', 7: 'left_elbow', 8: 'right_elbow',\n","          9: 'left_wrist', 10: 'right_wrist', 11: 'left_hip', 12: 'right_hip',\n","          13: 'left_knee', 14: 'right_knee', 15: 'left_ankle', 16: 'right_ankle',\n","          17: 'neck', 18: 'left_palm', 19: 'right_palm', 20: 'back_spine', 21: 'waist_spine',\n","          22: 'left_instep', 23: 'right_instep'\n","    }\n","\n","    self.joint_pair = [\n","          (0, 1), (0, 2), (2, 4), (1, 3), (6, 8), (8, 10),\n","          (5, 7), (7, 9), (11, 13), (13, 15), (12, 14), \n","          (14, 16), (5, 6), (15, 22), (16, 23), (11, 21),\n","          (21, 12), (20, 21), (5, 20), (6, 20), (17, 6), (17, 5)\n","    ]\n","\n","    self.flip_pair = [\n","          (1, 2), (3, 4), (5, 6), (7, 8),\n","          (9, 10), (11, 12), (13, 14), (15, 16),\n","          (18, 19), (22, 23)\n","    ]\n","\n","    \n","    cmap = plt.get_cmap(\"rainbow\")\n","    colors = [cmap(i) for i in np.linspace(0, 1,  self.num_joints + 2)]\n","    colors = [(c[2] * 255, c[1] * 255, c[0] * 255) for c in colors]\n","    self.joint_colors = {k: colors[k] for k in range(self.num_joints)}"]},{"cell_type":"markdown","metadata":{"id":"oRmxRieOCxnZ"},"source":["- top down 방식으로 pose estimation 진행하기 위해선 사람의 위치를 먼저 찾아줘야함.\n","- train image는 keypoint가 주어지기 때문에 위치를 파악하기 위해 별도의 모델을 사용할 필요X\n","- 아래 코드에서는 keypoint를 바탕으로 사람의 중심을 정의하고 이를 바탕으로 affine transformation하는 방식을 사용"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OY_IB6Mf3ZqL"},"outputs":[],"source":["# https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/blob/master/lib/utils/transforms.py\n","\n","def get_affine_transform(center,\n","                         scale,\n","                         rot,\n","                         output_size,\n","                         shift=np.array([0, 0], dtype=np.float32),\n","                         inv=0,):\n","    if not isinstance(scale, np.ndarray) and not isinstance(scale, list):\n","        print(scale)\n","        scale = np.array([scale, scale])\n","\n","    src_w = scale[0]\n","    dst_w = output_size[0]\n","    dst_h = output_size[1]\n","\n","    rot_rad = np.pi * rot / 180\n","    src_dir = get_dir([0, src_w * -0.5], rot_rad)\n","    dst_dir = np.array([0, dst_w * -0.5], np.float32)\n","\n","    src = np.zeros((3, 2), dtype=np.float32)\n","    dst = np.zeros((3, 2), dtype=np.float32)\n","    src[0, :] = center + scale * shift\n","    src[1, :] = center + src_dir + scale * shift\n","    dst[0, :] = [dst_w * 0.5, dst_h * 0.5]\n","    dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5]) + dst_dir\n","\n","    src[2:, :] = get_3rd_point(src[0, :], src[1, :])\n","    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])\n","\n","    if inv:\n","        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\n","    else:\n","        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n","\n","    return trans\n","\n","# 실제 어파인 변환 수행하는 부분\n","# t는 위의 get_affine_transform 함수를 통해 trans 매트릭스 구한다.\n","def affine_transform(pt, t):\n","    new_pt = np.array([pt[0], pt[1], 1.]).T\n","    new_pt = np.dot(t, new_pt)\n","    return new_pt[:2]\n","\n","\n","def get_dir(src_point, rot_rad):\n","    \"\"\" \n","        Transformation Matrix \n","        x = x * cosΘ - y * sinΘ\n","        y = x * cosΘ + y * sinΘ\n","        [ cosΘ   sinΘ   0]\n","        [ -sinΘ  cosΘ   0]\n","        [  0       0    0]\n","    \"\"\"\n","\n","    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\n","\n","    src_result = [0, 0]\n","    src_result[0] = src_point[0] * cs - src_point[1] * sn\n","    src_result[1] = src_point[0] * sn + src_point[1] * cs\n","\n","    return src_result\n","\n","\n","def get_3rd_point(a, b):\n","    direct = a - b\n","    return b + np.array([-direct[1], direct[0]], dtype=np.float32)"]},{"cell_type":"markdown","metadata":{"id":"LfRcWh7lC35j"},"source":["UDP 방식으로 data preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68Z1SmrO3ZqN"},"outputs":[],"source":["class DaconKeypointsDataset(Dataset):\n","    def __init__(\n","        self,\n","        cfg: SingleModelConfig,\n","        image_dir: str, \n","        label_df: pd.DataFrame, \n","        transforms: Sequence = None,\n","        mode: str = 'train'\n","    ) -> None:\n","        self.image_dir = image_dir\n","        self.df = label_df\n","        self.transforms = transforms\n","\n","        self.mode = mode\n","        self.kpd = cfg.kpd\n","        self.debug = cfg.debug\n","        self.shift = cfg.shift\n","        self.num_joints = cfg.num_joints\n","        self.flip_pairs = cfg.flip_pair\n","        self.image_size = cfg.image_size\n","        self.heatmap_size = cfg.output_size\n","        self.sigma = cfg.sigma\n","        self.target_type = cfg.target_type\n","\n","\n","\n","    def __len__(self) -> int:\n","        return self.df.shape[0]\n","    \n","    def __getitem__(self, index: int):\n","        image_id = self.df.iloc[index, 0]\n","\n","        labels = np.array([1])\n","        keypoints = self.df.iloc[index, 1:].values.reshape(-1, 2).astype(np.float32)\n","        keypoints = np.concatenate([keypoints,  np.ones((24, 1))], axis=1)\n","\n","        # define bbox\n","        xmin = np.min(keypoints[:, 0])\n","        xmax = np.max(keypoints[:, 0])\n","        width = xmax - xmin if xmax > xmin else 20\n","        center = (xmin + xmax)/2.\n","        xmin = int(center - width/2.*1.2)\n","        xmax = int(center + width/2.*1.2)\n","\n","        ymin = np.min(keypoints[:, 1])\n","        ymax = np.max(keypoints[:, 1])\n","        height = ymax - ymin if ymax > ymin else 20\n","        center = (ymin + ymax)/2.\n","        ymin = int(center - height/2.*1.2)\n","        ymax = int(center + height/2.*1.2)\n","        \n","\n","        x, y, w, h = xmin, ymin, xmax-xmin, ymax-ymin\n","        aspect_ratio = self.image_size[1] / self.image_size[0]\n","        centre = np.array([x+w*.5, y+h*.5])\n","        if w > aspect_ratio * h:\n","            h = w * 1.0 / aspect_ratio\n","        # if w < aspect_ratio * h:\n","        elif w < aspect_ratio * h:\n","            w = h * aspect_ratio\n","        \n","        scale = np.array([w, h]) * 1.25\n","        rotation = 0\n","\n","        image = cv2.imread(os.path.join(self.image_dir, image_id), cv2.COLOR_BGR2RGB)\n","          \n","        # if it's train mode\n","        if self.mode == 'train':\n","            scale_factor = 0.3\n","            rotate_factor = 45\n","            scale = scale * np.clip(np.random.randn()*scale_factor+1,1-scale_factor, 1+scale_factor)\n","            rotation = np.clip(np.random.randn()*rotate_factor, -rotate_factor*2, rotate_factor*2) if random.random() <= 0.5 else 0\n","            \n","\n","            # lr flipping\n","            if np.random.random() <= 0.5:\n","              image = np.flip(image, 1)\n","              centre[0] = image.shape[1] - 1 - centre[0]\n","\n","              keypoints[:, 0] = image.shape[1] - 1 - keypoints[:, 0]\n","              for (q, w) in self.flip_pairs:\n","                  keypoints_q, keypoints_w = keypoints[q, :].copy(), keypoints[w, :].copy()\n","                  keypoints[w, :], keypoints[q, :] = keypoints_q, keypoints_w\n","            \n","\n","        trans = get_affine_transform(centre, scale, rotation, (self.image_size[1], self.image_size[0]))\n","        cropped_image = cv2.warpAffine(image, trans, (self.image_size[1], self.image_size[0]), flags=cv2.INTER_LINEAR)\n","        for j in range(self.num_joints):\n","            if keypoints[j, 2] > 0:\n","                keypoints[j, :2] = affine_transform(keypoints[j, :2], trans)\n","                keypoints[j, 2] *= ((keypoints[j, 0] >= 0) & (keypoints[j, 0] < self.image_size[1]) \\\n","                                  & (keypoints[j, 1] >= 0) & (keypoints[j, 1] < self.image_size[0]))\n","        \n","        target, target_weight = self.generate_target(keypoints[:, :2], keypoints[:, 2])\n","        target = torch.from_numpy(target)\n","        target_weight = torch.from_numpy(target_weight)\n","\n","        if self.transforms is not None:\n","            cropped_image = self.transforms(image=cropped_image)['image']\n","\n","        # random horizontal & vertical shifting\n","        if self.mode=='train' and self.shift and np.random.random() <= 0.5:\n","              cropped_image, keypoints = self.shift_images(cropped_image, keypoints)\n","\n","\n","        if self.debug:\n","          show_image(cropped_image, keypoints)\n","\n","\n","          target_heatmap = self.render_gaussian_heatmap(keypoints[:, :2], output_shape=self.heatmap_size)\n","          visualize_heatmap = target_heatmap #* 255.\n","          visualize_heatmap = visualize_heatmap.astype('uint8')[0]\n","          visualize_heatmap = np.max(visualize_heatmap, axis=2)\n","          visualize_heatmap = cv2.applyColorMap(visualize_heatmap, cv2.COLORMAP_JET)\n","          fig, ax = plt.subplots(dpi=200)\n","          ax.imshow(visualize_heatmap)\n","          ax.axis('off')\n","          plt.show()\n","\n","\n","        sample = {\n","                  'image': torch.from_numpy(cropped_image).float().permute(2, 0, 1),\n","                  'keypoints': torch.from_numpy(keypoints).float(),\n","                  'target': target,\n","                  'target_weight': target_weight\n","                 }\n","        return sample\n","    \n","    def shift_images(self, image, keypoints, max_v=25, max_h=25):\n","        shift_v = np.random.randint(low=-max_v, high=max_v, size=1)\n","        shift_h = np.random.randint(low=-max_h, high=max_h, size=1)\n","\n","        m = np.array([\n","            [1, 0, shift_h],\n","            [0, 1, shift_v]\n","        ]).astype(np.float32)\n","        \n","        rows, cols = image.shape[:-1]\n","        image = cv2.warpAffine(image, m, (cols, rows))\n","\n","        for j in range(len(keypoints)):\n","            if keypoints[j, 2] > 0:\n","                  keypoints[j, :2] = affine_transform(keypoints[j, :2], m)\n","                  keypoints[j, 2] *= ((keypoints[j, 0] >= 0) & (keypoints[j, 0] < self.image_size[1]) \\\n","                                    & (keypoints[j, 1] >= 0) & (keypoints[j, 1] < self.image_size[0]))\n","        return image, keypoints\n","\n","    # https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/blob/master/lib/dataset/JointsDataset.py\n","    # heatmap과 offset에 대해 설명해놓은 글 참고: https://ivdevlog.tistory.com/2\n","    \n","    def generate_target(self, joints, joints_vis):\n","        '''\n","        :param joints:  [num_joints, 3]\n","        :param joints_vis: [num_joints, 3]\n","        :return: target, target_weight(1: visible, 0: invisible)\n","        '''\n","        target_weight = np.ones((self.num_joints, 1), dtype=np.float32)\n","        target_weight[:, 0] = joints_vis\n","\n","        \n","        target = np.zeros((self.num_joints,\n","                          self.heatmap_size[0],\n","                          self.heatmap_size[1]),\n","                         dtype=np.float32)\n","        tmp_size = self.sigma * 3\n","\n","        for joint_id in range(self.num_joints):\n","          feat_stride = self.image_size / self.heatmap_size\n","          mu_x = int(joints[joint_id][0] / feat_stride[0] + 0.5)\n","          mu_y = int(joints[joint_id][1] / feat_stride[1] + 0.5)\n","          # Check that any part of the gaussian is in-bounds\n","          ul = [int(mu_x - tmp_size), int(mu_y - tmp_size)]\n","          br = [int(mu_x + tmp_size + 1), int(mu_y + tmp_size + 1)]\n","          if ul[0] >= self.heatmap_size[1] or ul[1] >= self.heatmap_size[0] or br[0] < 0 or br[1] < 0:\n","            target_weight[joint_id] = 0\n","            continue\n","\n","        # # Generate gaussian\n","        size = 2 * tmp_size + 1\n","        x = np.arange(0, size, 1, np.float32)\n","        y = x[:, np.newaxis]\n","        x0 = y0 = size // 2\n","        # The gaussian is not normalized, we want the center value to equal 1\n","        g = np.exp(- ((x - x0) ** 2 + (y - y0) ** 2) / (2 * self.sigma ** 2))\n","\n","        # Usable gaussian range\n","        g_x = max(0, -ul[0]), min(br[0], self.heatmap_size[1]) - ul[0]\n","        g_y = max(0, -ul[1]), min(br[1], self.heatmap_size[0]) - ul[1]\n","        # Image range\n","        img_x = max(0, ul[0]), min(br[0], self.heatmap_size[1])\n","        img_y = max(0, ul[1]), min(br[1], self.heatmap_size[0])\n","\n","        v = target_weight[joint_id]\n","        if v > 0.5:\n","          target[joint_id][img_y[0]:img_y[1], img_x[0]:img_x[1]] = g[g_y[0]:g_y[1], g_x[0]:g_x[1]]\n","\n","        return target, target_weight"]},{"cell_type":"markdown","metadata":{"id":"j76UgWN_EN-0"},"source":["- 최종 예측을 위한 함수 정의\n","  - dark-pose https://github.com/ilovepose/DarkPose/blob/master/lib/core/inference.py\n","  - get_max_preds\n","  - get_final_preds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ulhUmP1h3ZqW"},"outputs":[],"source":["# https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/blob/ba50a82dce412df97f088c572d86d7977753bf74/lib/core/inference.py#L18:5\n","\n","from numpy.linalg import LinAlgError\n","\n","def get_max_preds(batch_heatmaps):\n","    '''\n","    get predictions from score maps\n","    heatmaps: numpy.ndarray([batch_size, num_joints, height, width])\n","    '''\n","    assert isinstance(batch_heatmaps, np.ndarray), \\\n","        'batch_heatmaps should be numpy.ndarray'\n","    assert batch_heatmaps.ndim == 4, 'batch_images should be 4-ndim'\n","\n","    batch_size = batch_heatmaps.shape[0]\n","    num_joints = batch_heatmaps.shape[1]\n","    width = batch_heatmaps.shape[3]\n","    heatmaps_reshaped = batch_heatmaps.reshape((batch_size, num_joints, -1))\n","    idx = np.argmax(heatmaps_reshaped, 2)\n","    maxvals = np.amax(heatmaps_reshaped, 2)\n","\n","    maxvals = maxvals.reshape((batch_size, num_joints, 1))\n","    idx = idx.reshape((batch_size, num_joints, 1))\n","\n","    preds = np.tile(idx, (1, 1, 2)).astype(np.float32)\n","\n","    preds[:, :, 0] = (preds[:, :, 0]) % width\n","    preds[:, :, 1] = np.floor((preds[:, :, 1]) / width)\n","\n","    pred_mask = np.tile(np.greater(maxvals, 0.0), (1, 1, 2))\n","    pred_mask = pred_mask.astype(np.float32)\n","\n","    preds *= pred_mask\n","    return preds, maxvals\n","\n","def dark_post_processing(coords,batch_heatmaps):\n","    '''\n","    DARK post-pocessing\n","    :param coords: batchsize*num_kps*2\n","    :param batch_heatmaps:batchsize*num_kps*high*width\n","    :return:\n","    '''\n","\n","    shape_pad = list(batch_heatmaps.shape)\n","    shape_pad[2] = shape_pad[2] + 2\n","    shape_pad[3] = shape_pad[3] + 2\n","\n","    for i in range(shape_pad[0]):\n","        for j in range(shape_pad[1]):\n","            mapij=batch_heatmaps[i,j,:,:]\n","            maxori = np.max(mapij)\n","            mapij= cv2.GaussianBlur(mapij,(7, 7), 0)\n","            max = np.max(mapij)\n","            min = np.min(mapij)\n","            mapij = (mapij-min)/(max-min) * maxori\n","            batch_heatmaps[i, j, :, :]= mapij\n","    batch_heatmaps = np.clip(batch_heatmaps,0.001,50)\n","    batch_heatmaps = np.log(batch_heatmaps)\n","    batch_heatmaps_pad = np.zeros(shape_pad,dtype=float)\n","    batch_heatmaps_pad[:, :, 1:-1,1:-1] = batch_heatmaps\n","    batch_heatmaps_pad[:, :, 1:-1, -1] = batch_heatmaps[:, :, :,-1]\n","    batch_heatmaps_pad[:, :, -1, 1:-1] = batch_heatmaps[:, :, -1, :]\n","    batch_heatmaps_pad[:, :, 1:-1, 0] = batch_heatmaps[:, :, :, 0]\n","    batch_heatmaps_pad[:, :, 0, 1:-1] = batch_heatmaps[:, :, 0, :]\n","    batch_heatmaps_pad[:, :, -1, -1] = batch_heatmaps[:, :, -1 , -1]\n","    batch_heatmaps_pad[:, :, 0, 0] = batch_heatmaps[:, :, 0, 0]\n","    batch_heatmaps_pad[:, :, 0, -1] = batch_heatmaps[:, :, 0, -1]\n","    batch_heatmaps_pad[:, :, -1, 0] = batch_heatmaps[:, :, -1, 0]\n","    I = np.zeros((shape_pad[0],shape_pad[1]))\n","    Ix1 = np.zeros((shape_pad[0], shape_pad[1]))\n","    Iy1 = np.zeros((shape_pad[0], shape_pad[1]))\n","    Ix1y1 = np.zeros((shape_pad[0],shape_pad[1]))\n","    Ix1_y1_ = np.zeros((shape_pad[0], shape_pad[1]))\n","    Ix1_ = np.zeros((shape_pad[0], shape_pad[1]))\n","    Iy1_ = np.zeros((shape_pad[0], shape_pad[1]))\n","    coords = coords.astype(np.int32)\n","    for i in range(shape_pad[0]):\n","        for j in range(shape_pad[1]):\n","            I[i, j] = batch_heatmaps_pad[i, j, coords[i, j, 1]+1, coords[i, j, 0]+1]\n","            Ix1[i, j] = batch_heatmaps_pad[i, j, coords[i, j, 1]+1, coords[i, j, 0] + 2]\n","            Ix1_[i, j] = batch_heatmaps_pad[i, j, coords[i, j, 1]+1, coords[i, j, 0] ]\n","            Iy1[i, j] = batch_heatmaps_pad[i, j, coords[i, j, 1] + 2, coords[i, j, 0]+1]\n","            Iy1_[i, j] = batch_heatmaps_pad[i, j, coords[i, j, 1] , coords[i, j, 0]+1]\n","            Ix1y1[i, j] = batch_heatmaps_pad[i, j, coords[i, j, 1] + 2, coords[i, j, 0] + 2]\n","            Ix1_y1_[i, j] = batch_heatmaps_pad[i, j, coords[i, j, 1], coords[i, j, 0]]\n","    dx = 0.5 * (Ix1 -  Ix1_)\n","    dy = 0.5 * (Iy1 - Iy1_)\n","    D = np.zeros((shape_pad[0],shape_pad[1],2))\n","    D[:,:,0]=dx\n","    D[:,:,1]=dy\n","    D.reshape((shape_pad[0],shape_pad[1],2,1))\n","    dxx = Ix1 - 2*I + Ix1_\n","    dyy = Iy1 - 2*I + Iy1_\n","    dxy = 0.5*(Ix1y1- Ix1 -Iy1 + I + I -Ix1_-Iy1_+Ix1_y1_)\n","    hessian = np.zeros((shape_pad[0],shape_pad[1],2,2))\n","    hessian[:, :, 0, 0] = dxx\n","    hessian[:, :, 1, 0] = dxy\n","    hessian[:, :, 0, 1] = dxy\n","    hessian[:, :, 1, 1] = dyy\n","    inv_hessian = np.zeros(hessian.shape)\n","    # hessian_test = np.zeros(hessian.shape)\n","    for i in range(shape_pad[0]):\n","        for j in range(shape_pad[1]):\n","            hessian_tmp = hessian[i,j,:,:]\n","            try:\n","                inv_hessian[i,j,:,:] = np.linalg.inv(hessian_tmp)\n","            except LinAlgError:\n","                inv_hessian[i, j, :, :] = np.zeros((2,2))\n","            # hessian_test[i,j,:,:] = np.matmul(hessian[i,j,:,:],inv_hessian[i,j,:,:])\n","            # print( hessian_test[i,j,:,:])\n","    res = np.zeros(coords.shape)\n","    coords = coords.astype(np.float)\n","    for i in range(shape_pad[0]):\n","        for j in range(shape_pad[1]):\n","            D_tmp = D[i,j,:]\n","            D_tmp = D_tmp[:,np.newaxis]\n","            shift = np.matmul(inv_hessian[i,j,:,:],D_tmp)\n","            # print(shift.shape)\n","            res_tmp = coords[i, j, :] -  shift.reshape((-1))\n","            res[i,j,:] = res_tmp\n","    return res\n","\n","\n","def get_final_preds(cfg, batch_heatmaps):\n","    heatmap_height = batch_heatmaps.shape[2]\n","    heatmap_width = batch_heatmaps.shape[3]\n","    if cfg.target_type == 'gaussian':\n","        coords, maxvals = get_max_preds(batch_heatmaps)\n","        if cfg.post_processing == \"dark\":\n","            coords = dark_post_processing(coords,batch_heatmaps)\n","    \n","    preds = coords.copy()\n","    preds[:,:, 0] = preds[:,:, 0] / (heatmap_width - 1.0) * (4 * heatmap_width - 1.0)\n","    preds[:,:, 1] = preds[:,:, 1] / (heatmap_height - 1.0) * (4 * heatmap_height - 1.0)\n","\n","    return preds"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"it74n9UHrFn_"},"outputs":[],"source":["# https://github.com/leoxiaobin/deep-high-resolution-net.pytorch/blob/ba50a82dce412df97f088c572d86d7977753bf74/lib/core/evaluate.py#L41\n","\n","def calc_dists(preds, target, normalize):\n","    preds = preds.astype(np.float32)\n","    target = target.astype(np.float32)\n","    dists = np.zeros((preds.shape[1], preds.shape[0]))\n","    for n in range(preds.shape[0]):\n","        for c in range(preds.shape[1]):\n","            if target[n, c, 0] > 1 and target[n, c, 1] > 1:\n","                normed_preds = preds[n, c, :] / normalize[n]\n","                normed_targets = target[n, c, :] / normalize[n]\n","                dists[c, n] = np.linalg.norm(normed_preds - normed_targets)\n","            else:\n","                dists[c, n] = -1\n","    return dists\n","\n","\n","def dist_acc(dists, thr=0.5):\n","    ''' Return percentage below threshold while ignoring values with a -1 '''\n","    dist_cal = np.not_equal(dists, -1)\n","    num_dist_cal = dist_cal.sum()\n","    if num_dist_cal > 0:\n","        return np.less(dists[dist_cal], thr).sum() * 1.0 / num_dist_cal\n","    else:\n","        return -1\n","\n","def accuracy(output, target, hm_type='gaussian', thr=0.5):\n","    '''\n","    Calculate accuracy according to PCK,\n","    but uses ground truth heatmap rather than x,y locations\n","    First value to be returned is average accuracy across 'idxs',\n","    followed by individual accuracies\n","    '''\n","    idx = list(range(output.shape[1]))\n","    norm = 1.0\n","    if hm_type == 'gaussian':\n","        pred, _ = get_max_preds(output)\n","        target, _ = get_max_preds(target)\n","        h = output.shape[2]\n","        w = output.shape[3]\n","        norm = np.ones((pred.shape[0], 2)) * np.array([h, w]) / 10\n","    dists = calc_dists(pred, target, norm)\n","\n","    acc = np.zeros((len(idx) + 1))\n","    avg_acc = 0\n","    cnt = 0\n","\n","    for i in range(len(idx)):\n","        acc[i + 1] = dist_acc(dists[idx[i]])\n","        if acc[i + 1] >= 0:\n","            avg_acc = avg_acc + acc[i + 1]\n","            cnt += 1\n","\n","    avg_acc = avg_acc / cnt if cnt != 0 else 0\n","    if cnt != 0:\n","        acc[0] = avg_acc\n","    return acc, avg_acc, cnt, pred"]},{"cell_type":"markdown","metadata":{"id":"Tztm6pV1Eoth"},"source":["- Loss 함수 정의\n","  - JointsRMSELoss\n","  - HeatmapMSELoss\n","  - HeatmapOHKMMSELoss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DGM5yl6fjMaB"},"outputs":[],"source":["class JointsRMSELoss(nn.Module):\n","    def __init__(self, use_target_weight=True):\n","        super(JointsRMSELoss, self).__init__()\n","        self.use_target_weight = use_target_weight\n","        self.criterion = nn.MSELoss(reduction='none')\n","\n","    def forward(self, pred, target):\n","        target_coord = target[:, :, :2]\n","        target_weight = target[:, :, 2].unsqueeze(-1)\n","\n","        loss = self.criterion(pred, target_coord)\n","        if self.use_target_weight:\n","          loss *= target_weight\n","          \n","        loss = torch.sqrt(torch.mean(torch.mean(loss, dim=0)))\n","        return loss\n","\n","\n","class HeatmapMSELoss(nn.Module):\n","    def __init__(self, use_target_weight=True):\n","        super(HeatmapMSELoss, self).__init__()\n","        self.criterion = nn.MSELoss(reduction='mean')\n","        self.use_target_weight = use_target_weight\n","\n","    def forward(self, output, target, target_weight):\n","        batch_size = output.size(0)\n","        num_joints = output.size(1)\n","        heatmaps_pred = output.reshape((batch_size, num_joints, -1)).split(1, 1)\n","        heatmaps_gt = target.reshape((batch_size, num_joints, -1)).split(1, 1)\n","\n","        loss = 0\n","\n","        for idx in range(num_joints):\n","            heatmap_pred = heatmaps_pred[idx].squeeze()\n","            heatmap_gt = heatmaps_gt[idx].squeeze()\n","\n","            if self.use_target_weight:\n","                loss += 0.5 * self.criterion(\n","                    heatmap_pred.mul(target_weight[:, idx]),\n","                    heatmap_gt.mul(target_weight[:, idx])\n","                )\n","            else:\n","                loss += 0.5 * self.criterion(heatmap_pred, heatmap_gt)\n","\n","        return loss / num_joints\n","\n","\n","class HeatmapOHKMMSELoss(nn.Module):\n","    def __init__(self, use_target_weight=True, topk=8):\n","        super(HeatmapOHKMMSELoss, self).__init__()\n","        self.criterion = nn.MSELoss(reduction='none')\n","        self.use_target_weight = use_target_weight\n","        self.topk = topk\n","\n","    def ohkm(self, loss):\n","        ohkm_loss = 0.\n","        for i in range(loss.size()[0]):\n","            sub_loss = loss[i]\n","            topk_val, topk_idx = torch.topk(\n","                sub_loss, k=self.topk, dim=0, sorted=False\n","            )\n","            tmp_loss = torch.gather(sub_loss, 0, topk_idx)\n","            ohkm_loss += torch.sum(tmp_loss) / self.topk\n","        ohkm_loss /= loss.size()[0]\n","        return ohkm_loss\n","\n","    def forward(self, output, target, target_weight):\n","        batch_size = output.size(0)\n","        num_joints = output.size(1)\n","        heatmaps_pred = output.reshape((batch_size, num_joints, -1)).split(1, 1)\n","        heatmaps_gt = target.reshape((batch_size, num_joints, -1)).split(1, 1)\n","\n","        loss = []\n","        for idx in range(num_joints):\n","            heatmap_pred = heatmaps_pred[idx].squeeze()\n","            heatmap_gt = heatmaps_gt[idx].squeeze()\n","            if self.use_target_weight:\n","                loss.append(0.5 * self.criterion(\n","                    heatmap_pred.mul(target_weight[:, idx]),\n","                    heatmap_gt.mul(target_weight[:, idx])\n","                ))\n","            else:\n","                loss.append(\n","                    0.5 * self.criterion(heatmap_pred, heatmap_gt)\n","                )\n","\n","        loss = [l.sum(dim=1).unsqueeze(dim=1) for l in loss]\n","        loss = torch.cat(loss, dim=1)\n","\n","        return self.ohkm(loss)"]},{"cell_type":"markdown","metadata":{"id":"TfYfPnLaFNQh"},"source":["모델(HRNEt) 정의"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WAWVzLnSMDGT"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","\n","BN_MOMENTUM = 0.1\n","\n","\n","def conv3x3(in_planes, out_planes, stride=1):\n","    \"\"\"3x3 convolution with padding\"\"\"\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = conv3x3(inplanes, planes, stride)\n","        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.conv2 = conv3x3(planes, planes)\n","        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n","        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(planes * self.expansion, momentum=BN_MOMENTUM)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        residual = x\n","\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        out = self.relu(out)\n","\n","        out = self.conv3(out)\n","        out = self.bn3(out)\n","\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","\n","class HighResolutionModule(nn.Module):\n","    def __init__(self, num_branches, blocks, num_blocks, num_inchannels, num_channels, multi_scale_output=True):\n","        super(HighResolutionModule, self).__init__()\n","        self._check_branches(num_branches, blocks, num_blocks, num_inchannels, num_channels)\n","\n","        self.num_inchannels = num_inchannels\n","        self.num_branches = num_branches\n","\n","        self.multi_scale_output = multi_scale_output\n","\n","        self.branches = self._make_branches(num_branches, blocks, num_blocks, num_channels)\n","        self.fuse_layers = self._make_fuse_layers()\n","        self.relu = nn.ReLU(True)\n","\n","    def _check_branches(self, num_branches, blocks, num_blocks, num_inchannels, num_channels):\n","        if num_branches != len(num_blocks):\n","            error_msg = \"NUM_BRANCHES({}) <> NUM_BLOCKS({})\".format(num_branches, len(num_blocks))\n","            raise ValueError(error_msg)\n","\n","        if num_branches != len(num_channels):\n","            error_msg = \"NUM_BRANCHES({}) <> NUM_CHANNELS({})\".format(num_branches, len(num_channels))\n","            raise ValueError(error_msg)\n","\n","        if num_branches != len(num_inchannels):\n","            error_msg = \"NUM_BRANCHES({}) <> NUM_INCHANNELS({})\".format(num_branches, len(num_inchannels))\n","            raise ValueError(error_msg)\n","\n","    def _make_one_branch(self, branch_index, block, num_blocks, num_channels, stride=1):\n","        downsample = None\n","        if stride != 1 or self.num_inchannels[branch_index] != num_channels[branch_index] * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(\n","                    self.num_inchannels[branch_index],\n","                    num_channels[branch_index] * block.expansion,\n","                    kernel_size=1,\n","                    stride=stride,\n","                    bias=False,\n","                ),\n","                nn.BatchNorm2d(num_channels[branch_index] * block.expansion, momentum=BN_MOMENTUM),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.num_inchannels[branch_index], num_channels[branch_index], stride, downsample))\n","        self.num_inchannels[branch_index] = num_channels[branch_index] * block.expansion\n","        for i in range(1, num_blocks[branch_index]):\n","            layers.append(block(self.num_inchannels[branch_index], num_channels[branch_index]))\n","\n","        return nn.Sequential(*layers)\n","\n","    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n","        branches = []\n","\n","        for i in range(num_branches):\n","            branches.append(self._make_one_branch(i, block, num_blocks, num_channels))\n","\n","        return nn.ModuleList(branches)\n","\n","    def _make_fuse_layers(self):\n","        if self.num_branches == 1:\n","            return None\n","\n","        num_branches = self.num_branches\n","        num_inchannels = self.num_inchannels\n","        fuse_layers = []\n","        for i in range(num_branches if self.multi_scale_output else 1):\n","            fuse_layer = []\n","            for j in range(num_branches):\n","                if j > i:\n","                    fuse_layer.append(\n","                        nn.Sequential(\n","                            nn.Conv2d(num_inchannels[j], num_inchannels[i], 1, 1, 0, bias=False),\n","                            nn.BatchNorm2d(num_inchannels[i]),\n","                            nn.Upsample(scale_factor=2 ** (j - i), mode=\"nearest\"),\n","                        )\n","                    )\n","                elif j == i:\n","                    fuse_layer.append(None)\n","                else:\n","                    conv3x3s = []\n","                    for k in range(i - j):\n","                        if k == i - j - 1:\n","                            num_outchannels_conv3x3 = num_inchannels[i]\n","                            conv3x3s.append(\n","                                nn.Sequential(\n","                                    nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False),\n","                                    nn.BatchNorm2d(num_outchannels_conv3x3),\n","                                )\n","                            )\n","                        else:\n","                            num_outchannels_conv3x3 = num_inchannels[j]\n","                            conv3x3s.append(\n","                                nn.Sequential(\n","                                    nn.Conv2d(num_inchannels[j], num_outchannels_conv3x3, 3, 2, 1, bias=False),\n","                                    nn.BatchNorm2d(num_outchannels_conv3x3),\n","                                    nn.ReLU(True),\n","                                )\n","                            )\n","                    fuse_layer.append(nn.Sequential(*conv3x3s))\n","            fuse_layers.append(nn.ModuleList(fuse_layer))\n","\n","        return nn.ModuleList(fuse_layers)\n","\n","    def get_num_inchannels(self):\n","        return self.num_inchannels\n","\n","    def forward(self, x):\n","        if self.num_branches == 1:\n","            return [self.branches[0](x[0])]\n","\n","        for i in range(self.num_branches):\n","            x[i] = self.branches[i](x[i])\n","\n","        x_fuse = []\n","\n","        for i in range(len(self.fuse_layers)):\n","            y = x[0] if i == 0 else self.fuse_layers[i][0](x[0])\n","            for j in range(1, self.num_branches):\n","                if i == j:\n","                    y = y + x[j]\n","                else:\n","                    y = y + self.fuse_layers[i][j](x[j])\n","            x_fuse.append(self.relu(y))\n","\n","        return x_fuse\n","\n","\n","class PoseHighResolutionNet(nn.Module):\n","    def __init__(self, width=32, num_keypoints=17):\n","        assert width in [32, 48], f\"PoseHighResolutionNet width must be in [32, 48] not {width}\"\n","        self.width = width\n","\n","        block = BasicBlock\n","        num_modules = [1, 4, 3]\n","        num_branches = [2, 3, 4]\n","        num_inchannels = [\n","            [2 ** i * width * block.expansion for i in range(2)],\n","            [2 ** i * width * block.expansion for i in range(3)],\n","            [2 ** i * width * block.expansion for i in range(4)],\n","        ]\n","        self.pre_stage_channels = [256]\n","\n","        self.inplanes = 64\n","        super(PoseHighResolutionNet, self).__init__()\n","\n","        # stem net\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n","        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.layer1 = self._make_layer(Bottleneck, 64, 4)\n","\n","        self.transition1 = self._make_transition_layer(num_inchannels[0])\n","        self.stage2 = self._make_stage(block, num_modules[0], num_branches[0], num_inchannels[0])\n","        self.transition2 = self._make_transition_layer(num_inchannels[1])\n","        self.stage3 = self._make_stage(block, num_modules[1], num_branches[1], num_inchannels[1])\n","        self.transition3 = self._make_transition_layer(num_inchannels[2])\n","        self.stage4 = self._make_stage(block, num_modules[2], num_branches[2], num_inchannels[2], multi_scale_output=False)\n","\n","        self.final_layer = nn.Conv2d(self.pre_stage_channels[0], num_keypoints, 1)\n","\n","        self.init_weights()\n","        self.num_branches = num_branches\n","\n","        self.finetune_step = 3\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.relu(x)\n","        x = self.layer1(x)\n","\n","        x_list = []\n","        for i in range(self.num_branches[0]):\n","            if self.transition1[i] is not None:\n","                x_list.append(self.transition1[i](x))\n","            else:\n","                x_list.append(x)\n","        y_list = self.stage2(x_list)\n","        x = y_list[-1]\n","\n","        x_list = []\n","        for i in range(self.num_branches[1]):\n","            if self.transition2[i] is not None:\n","                x_list.append(self.transition2[i](x))\n","            else:\n","                x_list.append(y_list[i])\n","        y_list = self.stage3(x_list)\n","        x = y_list[-1]\n","\n","        x_list = []\n","        for i in range(self.num_branches[2]):\n","            if self.transition3[i] is not None:\n","                x_list.append(self.transition3[i](x))\n","            else:\n","                x_list.append(y_list[i])\n","        y_list = self.stage4(x_list)\n","        x = y_list[0]\n","\n","        x = self.final_layer(x)\n","\n","        return x\n","\n","    def _make_transition_layer(self, num_channels_cur_layer):\n","        num_channels_pre_layer = self.pre_stage_channels\n","        num_branches_pre = len(num_channels_pre_layer)\n","        num_branches_cur = len(num_channels_cur_layer)\n","\n","        transition_layers = []\n","        for i in range(num_branches_cur):\n","            if i < num_branches_pre:\n","                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n","                    transition_layers.append(\n","                        nn.Sequential(\n","                            nn.Conv2d(num_channels_pre_layer[i], num_channels_cur_layer[i], 3, 1, 1, bias=False),\n","                            nn.BatchNorm2d(num_channels_cur_layer[i]),\n","                            nn.ReLU(inplace=True),\n","                        )\n","                    )\n","                else:\n","                    transition_layers.append(None)\n","            else:\n","                conv3x3s = []\n","                for j in range(i + 1 - num_branches_pre):\n","                    inchannels = num_channels_pre_layer[-1]\n","                    outchannels = num_channels_cur_layer[i] if j == i - num_branches_pre else inchannels\n","                    conv3x3s.append(\n","                        nn.Sequential(\n","                            nn.Conv2d(inchannels, outchannels, 3, 2, 1, bias=False),\n","                            nn.BatchNorm2d(outchannels),\n","                            nn.ReLU(inplace=True),\n","                        )\n","                    )\n","                transition_layers.append(nn.Sequential(*conv3x3s))\n","\n","        return nn.ModuleList(transition_layers)\n","\n","    def _make_layer(self, block, planes, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.inplanes != planes * block.expansion:\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n","            )\n","\n","        layers = []\n","        layers.append(block(self.inplanes, planes, stride, downsample))\n","        self.inplanes = planes * block.expansion\n","        for i in range(1, blocks):\n","            layers.append(block(self.inplanes, planes))\n","\n","        return nn.Sequential(*layers)\n","\n","    def _make_stage(self, block, num_module, num_branch, num_inchannels, multi_scale_output=True):\n","        modules = []\n","        for i in range(num_module):\n","            # multi_scale_output is only used last module\n","            if not multi_scale_output and i == num_module - 1:\n","                reset_multi_scale_output = False\n","            else:\n","                reset_multi_scale_output = True\n","\n","            modules.append(\n","                HighResolutionModule(\n","                    num_branch,\n","                    block,\n","                    [4 for _ in range(num_branch)],\n","                    num_inchannels,\n","                    [2 ** i * self.width for i in range(num_branch)],\n","                    reset_multi_scale_output,\n","                )\n","            )\n","            num_inchannels = modules[-1].get_num_inchannels()\n","\n","        self.pre_stage_channels = num_inchannels\n","        return nn.Sequential(*modules)\n","\n","    def init_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                nn.init.normal_(m.weight, std=0.001)\n","                for name, _ in m.named_parameters():\n","                    if name in [\"bias\"]:\n","                        nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.ConvTranspose2d):\n","                nn.init.normal_(m.weight, std=0.001)\n","                for name, _ in m.named_parameters():\n","                    if name in [\"bias\"]:\n","                        nn.init.constant_(m.bias, 0)\n","\n","    def freeze_step1(self):\n","        for p in self.parameters():\n","            p.requires_grad_(False)\n","        self.final_layer.requires_grad_(True)\n","        self.finetune_step = 1\n","\n","    def freeze_step2(self):\n","        for p in self.parameters():\n","            p.requires_grad_(True)\n","        self.final_layer.requires_grad_(False)\n","        self.finetune_step = 2\n","\n","    def freeze_step3(self):\n","        for p in self.parameters():\n","            p.requires_grad_(True)\n","        self.finetune_step = 3"]},{"cell_type":"markdown","metadata":{"id":"bCR5V826Fp87"},"source":["- 훈련 & 검증 method 정의"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WSBOPD-N3ZqY"},"outputs":[],"source":["def calc_coord_loss(pred, gt):\n","    batch_size = gt.size(0)\n","    valid_mask = gt[:, :, -1].view(batch_size, -1, 1)\n","    gt = gt[:, :, :2]\n","    return torch.mean(torch.sum(torch.abs(pred-gt) * valid_mask, dim=-1))\n","\n","def train(cfg, train_tfms=None, valid_tfms=None):\n","  # for reporduction\n","  seed = cfg.seed\n","  torch.cuda.empty_cache()\n","  seed_everything(2021)\n","\n","  # device type\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","  model = PoseHighResolutionNet(48)\n","  model.load_state_dict(torch.load('/content/drive/MyDrive/방학 CV분반 KUBIG CONTEST/임채명/pose_hrnet_w48_384x288.pth')) \n","\n","  final_layer = nn.Conv2d(48, 24, 1)\n","\n","  with torch.no_grad():\n","    final_layer.weight[:17] = model.final_layer.weight\n","    final_layer.bias[:17] = model.final_layer.bias\n","\n","    final_layer.weight[17] = model.final_layer.weight[[0, 5, 6]].clone().mean(0)\n","    final_layer.bias[17] = model.final_layer.bias[[0, 5, 6]].clone().mean(0)\n","    \n","    final_layer.weight[18] = model.final_layer.weight[9].clone()\n","    final_layer.bias[18] = model.final_layer.bias[9].clone()\n","    \n","    final_layer.weight[19] = model.final_layer.weight[10].clone()\n","    final_layer.bias[19] = model.final_layer.bias[10].clone()\n","    \n","    final_layer.weight[20] = model.final_layer.weight[[5, 6, 11, 12]].clone().mean(0)\n","    final_layer.bias[20] = model.final_layer.bias[[5, 6, 11, 12]].clone().mean(0)\n","    \n","    final_layer.weight[21] = torch.cat(\n","        (\n","            model.final_layer.weight[[11, 12]].clone() * 1 / 3,\n","            model.final_layer.weight[[5, 6]].clone() * 6 / 1,\n","        )\n","    ).mean(0)\n","    final_layer.bias[21] = torch.cat(\n","        (\n","            model.final_layer.bias[[11, 12]].clone() * 1 / 3,\n","            model.final_layer.bias[[5, 6]].clone() * 6 / 1,\n","        )\n","    ).mean(0)\n","    final_layer.weight[22] = model.final_layer.weight[15].clone()\n","    final_layer.bias[22] = model.final_layer.bias[15].clone()\n","\n","    final_layer.weight[23] = model.final_layer.weight[16].clone()\n","    final_layer.bias[23] = model.final_layer.bias[16].clone()\n","        \n","  model.final_layer = final_layer\n","\n","  model = model.to(device)\n"," \n","\n","  # define criterions\n","  #if cfg.target_type == \"gaussian\":\n","  #  if cfg.loss_type == \"MSE\":\n","  #    main_criterion = HeatmapMSELoss(True)\n","  #  elif cfg.loss_type == \"OHKMMSE\":\n","  #    main_criterion = HeatmapOHKMMSELoss(True)\n","\n","  main_criterion = nn.CrossEntropyLoss()\n","  rmse_criterion = JointsRMSELoss()\n","\n","  # define optimizer and scheduler\n","  optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n","\n","  # data read and add sector column for startify\n","  total_df = pd.read_csv(meta_info_dir)\n","  if not cfg.startify_with_dir:\n","    def making_sector_label(image_name):\n","        sector_name = image_name.split('-')[0]\n","        return sector_name\n","  else:\n","    def making_sector_label(image_name):\n","        pose = image_name.split('-')\n","        cam_dir = pose[4].split('_')[1]\n","        sector_name = pose[0] + cam_dir\n","        return sector_name\n","\n","  print(total_df.describe())\n","  total_df['sector'] = total_df.apply(\n","        lambda x: making_sector_label(x['image']), axis=1\n","  )\n","\n","  columns = total_df.columns.tolist()\n","  columns = columns[-1:] + columns[:-1]\n","  total_df = total_df[columns]\n","\n","\n","  # data prepare\n","  if cfg.startify:\n","    train_df, valid_df = train_test_split(total_df.iloc[:, 1:], test_size=cfg.test_ratio, random_state=seed, stratify=total_df.iloc[:, 0])\n","  else:\n","    train_df, valid_df = train_test_split(total_df.iloc[:, 1:], test_size=cfg.test_ratio, random_state=seed)\n","  \n","\n","  train_ds = DaconKeypointsDataset(cfg, train_img_path, train_df, train_tfms, mode='train')\n","  valid_ds  = DaconKeypointsDataset(cfg, train_img_path, valid_df, valid_tfms, mode='valid')\n","  train_dl = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True)\n","  valid_dl  = DataLoader(valid_ds, batch_size=cfg.batch_size, shuffle=False)\n","\n","  print(\"Train Transformation:\\n\", train_tfms, \"\\n\")\n","  print(\"Valid Transformation:\\n\", valid_tfms, \"\\n\")\n","\n","\n","  best_loss = float('INF')\n","  for epoch in range(cfg.epochs):\n","      ################\n","      #    Train     #\n","      ################\n","      with tqdm(train_dl, total=train_dl.__len__(), unit=\"batch\") as train_bar:\n","          train_acc_list = []\n","          train_rmse_list = []\n","          train_heatmap_list = []\n","          train_coord_list = []\n","          train_offset_list = []\n","          train_total_list = []\n","\n","          for sample in train_bar:\n","              train_bar.set_description(f\"Train Epoch {epoch+1}\")\n","\n","              optimizer.zero_grad()\n","              images, targ_coords = sample['image'].to(device), sample['keypoints'].to(device)\n","              target, target_weight = sample['target'].to(device), sample['target_weight'].to(device)\n","\n","              model.train()\n","              with torch.set_grad_enabled(True):\n","                  preds = model(images)\n","                  loss = main_criterion(preds, target)\n","#                  loss = main_criterion(preds, target, target_weight)\n","\n","                  heatmap_height = preds.shape[2]\n","                  heatmap_width = preds.shape[3]\n","                  pred_coords, _ = get_max_preds(preds.detach().cpu().numpy())\n","                  pred_coords[:, :, 0] = pred_coords[:, :, 0] / (heatmap_width - 1.0) * (4 * heatmap_width - 1.0)\n","                  pred_coords[:, :, 1] = pred_coords[:, :, 1] / (heatmap_height - 1.0) * (4 * heatmap_height - 1.0)\n","\n","                  pred_coords = torch.tensor(pred_coords).float().to(device)\n","                  coord_loss  = calc_coord_loss(pred_coords, targ_coords)\n","\n","                  rmse_loss = rmse_criterion(pred_coords, targ_coords)\n","                  _, avg_acc, cnt, pred = accuracy(preds.detach().cpu().numpy()[:, ::3, :, :],\n","                                                   target.detach().cpu().numpy()[:, ::3, :, :])\n","                  \n","                  loss.backward()\n","                  optimizer.step()\n","\n","                  train_rmse_list.append(rmse_loss.item())\n","                  train_total_list.append(loss.item())\n","                  train_coord_list.append(coord_loss.item())\n","                  train_acc_list.append(avg_acc)\n","              train_acc = np.mean(train_acc_list)\n","              train_rmse = np.mean(train_rmse_list)\n","              train_coord = np.mean(train_coord_list)\n","              train_total = np.mean(train_total_list)\n","\n","              train_bar.set_postfix(coord_loss = train_coord,\n","                                    rmse_loss = train_rmse,\n","                                    total_loss = train_total,\n","                                    train_acc  = train_acc)\n","      \n","      ################\n","      #    Valid     #\n","      ################\n","      with tqdm(valid_dl, total=valid_dl.__len__(), unit=\"batch\") as valid_bar:\n","          valid_acc_list = []\n","          valid_rmse_list = []\n","          valid_heatmap_list = []\n","          valid_coord_list = []\n","          valid_offset_list = []\n","          valid_total_list = []\n","          for sample in valid_bar:\n","              valid_bar.set_description(f\"Valid Epoch {epoch+1}\")\n","\n","              images, targ_coords = sample['image'].to(device), sample['keypoints'].to(device)\n","              target, target_weight = sample['target'].to(device), sample['target_weight'].to(device)\n","\n","              model.eval()\n","              with torch.no_grad():\n","                  preds = model(images)\n","                  loss = main_criterion(preds, target)\n","#                  loss = main_criterion(preds, target, target_weight)\n","                  \n","                  pred_coords = get_final_preds(cfg, preds.detach().cpu().numpy())\n","                  pred_coords = torch.tensor(pred_coords).float().to(device)\n","                  coord_loss  = calc_coord_loss(pred_coords, targ_coords)\n","\n","                  rmse_loss = rmse_criterion(pred_coords, targ_coords)\n","                  _, avg_acc, cnt, pred = accuracy(preds.detach().cpu().numpy()[:, ::3, :, :],\n","                                                   target.detach().cpu().numpy()[:, ::3, :, :])\n","                  \n","                  valid_rmse_list.append(rmse_loss.item())\n","                  valid_total_list.append(loss.item())\n","                  valid_coord_list.append(coord_loss.item())\n","                  valid_acc_list.append(avg_acc)\n","              valid_acc = np.mean(valid_acc_list)\n","              valid_rmse = np.mean(valid_rmse_list)\n","              valid_coord = np.mean(valid_coord_list)\n","              valid_total = np.mean(valid_total_list)\n","              valid_bar.set_postfix(coord_loss = valid_coord,\n","                                      rmse_loss = valid_rmse,\n","                                      total_loss = valid_total,\n","                                      valid_acc  = valid_acc)\n","\n","      if best_loss > valid_total:\n","          best_model = model\n","          save_dir = os.path.join(main_dir, cfg.save_folder)\n","          save_name = f'best_model.pth'\n","          torch.save(model.state_dict(), os.path.join(save_dir, save_name))\n","          print(f\"Valid Loss: {valid_total:.8f}\\nBest Model saved.\")\n","          best_loss = valid_total\n","  return best_model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mIjtTRLG3ZqZ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2480d5a5-5974-4d1a-e9c0-123a67c7a334","executionInfo":{"status":"ok","timestamp":1677481919007,"user_tz":-540,"elapsed":3556316,"user":{"displayName":"Claire Lim","userId":"01177983436534277283"}}},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/albumentations/augmentations/transforms.py:1639: FutureWarning: RandomContrast has been deprecated. Please use RandomBrightnessContrast\n","  warnings.warn(\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["            nose_x       nose_y   left_eye_x   left_eye_y  right_eye_x  \\\n","count  4094.000000  4094.000000  4094.000000  4094.000000  4094.000000   \n","mean    926.762676   467.327242   927.786692   460.544454   923.517024   \n","std     172.927768   138.818331   177.920833   143.639188   178.603650   \n","min     599.537515   236.000000   604.328953   228.000000   592.879610   \n","25%     779.939805   344.961634   779.221413   333.560038   772.511865   \n","50%     933.491697   467.266649   944.083046   462.029150   923.000307   \n","75%    1071.240718   579.000000  1064.550604   569.087374  1069.999213   \n","max    1312.405257   907.121424  1315.772361   893.906490  1332.979992   \n","\n","       right_eye_y   left_ear_x   left_ear_y  right_ear_x  right_ear_y  ...  \\\n","count  4094.000000  4094.000000  4094.000000  4094.000000  4094.000000  ...   \n","mean    460.885430   930.411870   465.535287   924.564418   466.156267  ...   \n","std     143.745995   167.722248   144.639208   168.536510   145.285203  ...   \n","min     228.000000   592.879610   237.000000   585.797372   239.000000  ...   \n","25%     334.000000   797.853787   340.255514   791.087206   342.000000  ...   \n","50%     457.456039   954.000000   444.742545   910.500000   446.121874  ...   \n","75%     574.214478  1040.058759   579.309922  1064.000000   573.080436  ...   \n","max     897.877879  1302.310980   865.214934  1316.528614   870.897418  ...   \n","\n","       right_palm_x  right_palm_y  spine2(back)_x  spine2(back)_y  \\\n","count   4094.000000   4094.000000     4094.000000     4094.000000   \n","mean     929.083488    591.195734      941.411438      530.110939   \n","std      160.501481    141.266154       97.305073       90.037893   \n","min      555.658256    177.582703      696.915595      344.461039   \n","25%      823.035066    527.252284      874.854467      458.079045   \n","50%      905.000000    598.000000      945.624215      513.203500   \n","75%     1049.244757    709.869606     1020.433504      588.452310   \n","max     1373.753285    949.000000     1165.000000      901.298674   \n","\n","       spine1(waist)_x  spine1(waist)_y  left_instep_x  left_instep_y  \\\n","count      4094.000000      4094.000000    4094.000000    4094.000000   \n","mean        950.926794       573.339010     987.555154     791.576792   \n","std          74.404416        65.111921     196.365967     133.486513   \n","min         738.774800       423.176270     522.715133     257.389455   \n","25%         891.835780       525.306645     837.096099     712.958881   \n","50%         967.000000       560.502949     987.000000     857.000000   \n","75%        1007.165842       608.591776    1087.034390     885.641261   \n","max        1159.000000       902.298674    1436.854600     967.000000   \n","\n","       right_instep_x  right_instep_y  \n","count     4094.000000     4094.000000  \n","mean       979.093452      789.846830  \n","std        201.630672      135.889479  \n","min        516.491925      237.187979  \n","25%        854.000000      711.137681  \n","50%        937.000000      846.000000  \n","75%       1086.175452      891.000000  \n","max       1440.204792      969.317127  \n","\n","[8 rows x 48 columns]\n","Train Transformation:\n"," Compose([\n","  GaussNoise(always_apply=False, p=0.5, var_limit=(10.0, 50.0), per_channel=True, mean=0),\n","  OneOf([\n","    MotionBlur(always_apply=False, p=1.0, blur_limit=(3, 15)),\n","    Blur(always_apply=False, p=1.0, blur_limit=(3, 7)),\n","    ImageCompression(always_apply=False, p=1.0, quality_lower=99, quality_upper=100, compression_type=0),\n","    GaussianBlur(always_apply=False, p=1.0, blur_limit=(3, 7), sigma_limit=(0, 0)),\n","  ], p=0.7),\n","  OneOf([\n","    ChannelShuffle(always_apply=False, p=1.0),\n","    HueSaturationValue(always_apply=False, p=1.0, hue_shift_limit=(-20, 20), sat_shift_limit=(-30, 30), val_shift_limit=(-20, 20)),\n","    RGBShift(always_apply=False, p=1.0, r_shift_limit=(-20, 20), g_shift_limit=(-20, 20), b_shift_limit=(-20, 20)),\n","  ], p=0.5),\n","  RandomBrightnessContrast(always_apply=False, p=0.6, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True),\n","  RandomContrast(always_apply=False, p=0.6, limit=(-0.2, 0.2)),\n","  RandomGamma(always_apply=False, p=0.6, gamma_limit=(80, 120), eps=None),\n","  CLAHE(always_apply=False, p=0.5, clip_limit=(1, 4.0), tile_grid_size=(8, 8)),\n","  Normalize(always_apply=False, p=1.0, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0),\n","], p=1.0, bbox_params=None, keypoint_params=None, additional_targets={}) \n","\n","Valid Transformation:\n"," Normalize(always_apply=False, p=1.0, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0) \n","\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["  0%|          | 0/435 [00:00<?, ?batch/s]<ipython-input-9-4b63d9693de0>:133: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n","  m = np.array([\n","Train Epoch 1: 100%|██████████| 435/435 [29:46<00:00,  4.11s/batch, coord_loss=246, rmse_loss=173, total_loss=0.000382, train_acc=0]\n","Valid Epoch 1:   0%|          | 0/77 [00:01<?, ?batch/s]<ipython-input-10-c0da3f4d6d85>:111: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  coords = coords.astype(np.float)\n","Valid Epoch 1: 100%|██████████| 77/77 [03:28<00:00,  2.70s/batch, coord_loss=445, rmse_loss=359, total_loss=1.48e-6, valid_acc=0]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Valid Loss: 0.00000148\n","Best Model saved.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Train Epoch 2: 100%|██████████| 435/435 [16:03<00:00,  2.22s/batch, coord_loss=444, rmse_loss=246, total_loss=5.1e-6, train_acc=0]\n","Valid Epoch 2: 100%|██████████| 77/77 [01:01<00:00,  1.25batch/s, coord_loss=436, rmse_loss=241, total_loss=4.87e-7, valid_acc=0]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Valid Loss: 0.00000049\n","Best Model saved.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Train Epoch 3: 100%|██████████| 435/435 [15:59<00:00,  2.21s/batch, coord_loss=454, rmse_loss=249, total_loss=5.72e-7, train_acc=0]\n","Valid Epoch 3: 100%|██████████| 77/77 [01:01<00:00,  1.25batch/s, coord_loss=457, rmse_loss=285, total_loss=6.64e-9, valid_acc=0]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Valid Loss: 0.00000001\n","Best Model saved.\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Train Epoch 4: 100%|██████████| 435/435 [16:02<00:00,  2.21s/batch, coord_loss=456, rmse_loss=250, total_loss=5e-8, train_acc=0]\n","Valid Epoch 4: 100%|██████████| 77/77 [01:02<00:00,  1.24batch/s, coord_loss=451, rmse_loss=246, total_loss=4.09e-9, valid_acc=0]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Valid Loss: 0.00000000\n","Best Model saved.\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 5: 100%|██████████| 435/435 [15:59<00:00,  2.21s/batch, coord_loss=456, rmse_loss=250, total_loss=7.14e-8, train_acc=0]\n","Valid Epoch 5: 100%|██████████| 77/77 [01:01<00:00,  1.25batch/s, coord_loss=450, rmse_loss=244, total_loss=8.75e-9, valid_acc=0]\n","Train Epoch 6: 100%|██████████| 435/435 [15:57<00:00,  2.20s/batch, coord_loss=455, rmse_loss=250, total_loss=1.98e-8, train_acc=0]\n","Valid Epoch 6: 100%|██████████| 77/77 [01:01<00:00,  1.25batch/s, coord_loss=450, rmse_loss=245, total_loss=4.67e-9, valid_acc=0]\n","Train Epoch 7: 100%|██████████| 435/435 [15:57<00:00,  2.20s/batch, coord_loss=457, rmse_loss=251, total_loss=6.08e-8, train_acc=0]\n","Valid Epoch 7: 100%|██████████| 77/77 [01:01<00:00,  1.25batch/s, coord_loss=453, rmse_loss=247, total_loss=5.47e-9, valid_acc=0]\n","Train Epoch 8: 100%|██████████| 435/435 [15:56<00:00,  2.20s/batch, coord_loss=457, rmse_loss=251, total_loss=1.86e-8, train_acc=0]\n","Valid Epoch 8: 100%|██████████| 77/77 [01:01<00:00,  1.25batch/s, coord_loss=453, rmse_loss=246, total_loss=9.98e-9, valid_acc=0]\n","Train Epoch 9: 100%|██████████| 435/435 [15:58<00:00,  2.20s/batch, coord_loss=459, rmse_loss=252, total_loss=9.36e-9, train_acc=0]\n","Valid Epoch 9: 100%|██████████| 77/77 [01:01<00:00,  1.26batch/s, coord_loss=452, rmse_loss=246, total_loss=7.67e-8, valid_acc=0]\n","Train Epoch 10: 100%|██████████| 435/435 [15:57<00:00,  2.20s/batch, coord_loss=459, rmse_loss=252, total_loss=5.86e-9, train_acc=0]\n","Valid Epoch 10: 100%|██████████| 77/77 [01:01<00:00,  1.25batch/s, coord_loss=453, rmse_loss=246, total_loss=5.26e-8, valid_acc=0]\n","Train Epoch 11: 100%|██████████| 435/435 [15:56<00:00,  2.20s/batch, coord_loss=459, rmse_loss=252, total_loss=3.44e-9, train_acc=0]\n","Valid Epoch 11: 100%|██████████| 77/77 [01:01<00:00,  1.24batch/s, coord_loss=454, rmse_loss=246, total_loss=2.67e-8, valid_acc=0]\n","Train Epoch 12: 100%|██████████| 435/435 [15:58<00:00,  2.20s/batch, coord_loss=465, rmse_loss=254, total_loss=1.83e-6, train_acc=0]\n","Valid Epoch 12: 100%|██████████| 77/77 [01:01<00:00,  1.25batch/s, coord_loss=469, rmse_loss=251, total_loss=7.42e-9, valid_acc=0]\n","Train Epoch 13: 100%|██████████| 435/435 [15:56<00:00,  2.20s/batch, coord_loss=476, rmse_loss=257, total_loss=6.18e-8, train_acc=0]\n","Valid Epoch 13: 100%|██████████| 77/77 [01:01<00:00,  1.26batch/s, coord_loss=470, rmse_loss=251, total_loss=2.33e-9, valid_acc=0]\n"]},{"output_type":"stream","name":"stdout","text":["Valid Loss: 0.00000000\n","Best Model saved.\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 14: 100%|██████████| 435/435 [15:58<00:00,  2.20s/batch, coord_loss=474, rmse_loss=256, total_loss=3.05e-9, train_acc=0]\n","Valid Epoch 14: 100%|██████████| 77/77 [01:01<00:00,  1.25batch/s, coord_loss=469, rmse_loss=251, total_loss=1.91e-9, valid_acc=0]\n"]},{"output_type":"stream","name":"stdout","text":["Valid Loss: 0.00000000\n","Best Model saved.\n"]},{"output_type":"stream","name":"stderr","text":["Train Epoch 15: 100%|██████████| 435/435 [15:58<00:00,  2.20s/batch, coord_loss=475, rmse_loss=257, total_loss=2.09e-9, train_acc=0]\n","Valid Epoch 15: 100%|██████████| 77/77 [01:01<00:00,  1.25batch/s, coord_loss=469, rmse_loss=251, total_loss=1.47e-9, valid_acc=0]\n"]},{"output_type":"stream","name":"stdout","text":["Valid Loss: 0.00000000\n","Best Model saved.\n"]}],"source":["n_tfms = A.Compose([\n","        A.GaussNoise(p=0.5),\n","\n","        A.OneOf([\n","            A.MotionBlur(p=1.0, blur_limit=15),\n","            A.Blur(p=1.0),\n","            A.ImageCompression(p=1.0),\n","            A.GaussianBlur(p=1.0),\n","        ], p=0.7),\n","\n","        A.OneOf([\n","            A.ChannelShuffle(p=1.0),\n","            A.HueSaturationValue(p=1.0),\n","            A.RGBShift(p=1.0),\n","        ], p=0.5),\n","\n","        A.RandomBrightnessContrast(p=0.6),\n","        A.RandomContrast(p=0.6),\n","        A.RandomGamma(p=0.6),\n","        A.CLAHE(p=0.5),\n","\n","        A.Normalize(p=1.0),\n","      ])\n","\n","valid_tfms = A.Normalize(p=1.0)\n","\n","cfg = SingleModelConfig(\n","    epochs=15,\n","    input_size=[512, 512], \n","    learning_rate=1e-3,\n","    sigma=3.0,\n","    batch_size=8,\n","\n","    shift = True,\n","    startify=True,\n","    init_training=True, \n","    startify_with_dir=True,\n","\n","    loss_type = \"MSE\",\n","    target_type = \"gaussian\",\n","    save_folder='/content/drive/MyDrive/방학 CV분반 KUBIG CONTEST/임채명/kubigcontestdata/result'\n","    )\n","\n","best_model = train(cfg, train_tfms=train_tfms, valid_tfms=valid_tfms)"]},{"cell_type":"markdown","metadata":{"id":"smU-iGUzQPyT"},"source":["\n","\n","\n","\n","\n","TEST: YOLOv5 써서 detection 먼저 수행"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8QxWTJlLT1Y9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677481995864,"user_tz":-540,"elapsed":11029,"user":{"displayName":"Claire Lim","userId":"01177983436534277283"}},"outputId":"96a90d03-7680-442b-a7b3-a3e14dff717a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'yolov5'...\n","remote: Enumerating objects: 15274, done.\u001b[K\n","remote: Counting objects: 100% (38/38), done.\u001b[K\n","remote: Compressing objects: 100% (33/33), done.\u001b[K\n","remote: Total 15274 (delta 11), reused 23 (delta 5), pack-reused 15236\u001b[K\n","Receiving objects: 100% (15274/15274), 14.17 MiB | 37.79 MiB/s, done.\n","Resolving deltas: 100% (10467/10467), done.\n","/content/yolov5\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting gitpython>=3.1.30\n","  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 6)) (3.5.3)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 7)) (1.22.4)\n","Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 8)) (4.6.0.66)\n","Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 9)) (7.1.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 10)) (5.4.8)\n","Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 11)) (6.0)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 12)) (2.25.1)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 13)) (1.7.3)\n","Collecting thop>=0.1.1\n","  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n","Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 15)) (1.13.1+cu116)\n","Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 16)) (0.14.1+cu116)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 17)) (4.64.1)\n","Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 21)) (2.11.2)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 26)) (1.3.5)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.8/dist-packages (from -r requirements.txt (line 27)) (0.11.2)\n","Collecting setuptools>=65.5.1\n","  Downloading setuptools-67.4.0-py3-none-any.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (1.4.4)\n","Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (2.8.2)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (4.38.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 6)) (23.0)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.23.0->-r requirements.txt (line 12)) (2022.12.7)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7.0->-r requirements.txt (line 15)) (4.5.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (2.16.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.4.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.51.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.38.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.8.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (3.4.1)\n","Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (3.19.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.6.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.4.6)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 26)) (2022.7.1)\n","Collecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.15.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (4.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (5.3.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 21)) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 21)) (6.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 21)) (3.14.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 21)) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 21)) (3.2.2)\n","Installing collected packages: smmap, setuptools, thop, gitdb, gitpython\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 57.4.0\n","    Uninstalling setuptools-57.4.0:\n","      Successfully uninstalled setuptools-57.4.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.9.0 requires jedi>=0.10, which is not installed.\n","cvxpy 1.2.3 requires setuptools<=64.0.2, but you have setuptools 67.4.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gitdb-4.0.10 gitpython-3.1.31 setuptools-67.4.0 smmap-5.0.0 thop-0.1.1.post2209072238\n"]}],"source":["!git clone https://github.com/ultralytics/yolov5\n","\n","%cd yolov5 \n","!pip install -r requirements.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"06hQQIZQKhI2","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["4719ccc2631c497a8054cab6dd472bef","e6b64c35642f4e4bbd4c04b5752ccf5c","df0230fc30544e8e84f8a28f3463ed73","25293b5e793e4b61a96d173e2360134c","24d568998cdb45d895de442f214a8ba4","8a4a25ea0510462b85c12664880e49c2","52b747683eb64e909261488ee8ecae70","7d7841719e304462b739991fd7550b30","be336f286abd481e84f9c01ff1cde7a0","99c1d651409b46419a3633fdd3fc3cda","e59d8372c4e34f308278fed88b3b2a3f"]},"executionInfo":{"status":"ok","timestamp":1677482009044,"user_tz":-540,"elapsed":13186,"user":{"displayName":"Claire Lim","userId":"01177983436534277283"}},"outputId":"0f0351b6-ba18-41dd-8e43-c772ba7400f4"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/torch/hub.py:267: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n","  warnings.warn(\n","Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n","\u001b[31m\u001b[1mrequirements:\u001b[0m YOLOv5 requirement \"setuptools>=65.5.1\" not found, attempting AutoUpdate...\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.8/dist-packages (67.4.0)\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m 1 package updated per /root/.cache/torch/hub/ultralytics_yolov5_master/requirements.txt\n","\u001b[31m\u001b[1mrequirements:\u001b[0m ⚠️ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","YOLOv5 🚀 2023-2-27 Python-3.8.10 torch-1.13.1+cu116 CUDA:0 (Tesla T4, 15102MiB)\n","\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5x.pt to yolov5x.pt...\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0.00/166M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4719ccc2631c497a8054cab6dd472bef"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\n","Fusing layers... \n","YOLOv5x summary: 444 layers, 86705005 parameters, 0 gradients\n","Adding AutoShape... \n"]},{"output_type":"execute_result","data":{"text/plain":["AutoShape(\n","  (model): DetectMultiBackend(\n","    (model): DetectionModel(\n","      (model): Sequential(\n","        (0): Conv(\n","          (conv): Conv2d(3, 80, kernel_size=(6, 6), stride=(2, 2), padding=(2, 2))\n","          (act): SiLU(inplace=True)\n","        )\n","        (1): Conv(\n","          (conv): Conv2d(80, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (2): C3(\n","          (cv1): Conv(\n","            (conv): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(160, 80, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv3): Conv(\n","            (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (m): Sequential(\n","            (0): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (1): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (2): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (3): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(80, 80, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(80, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","          )\n","        )\n","        (3): Conv(\n","          (conv): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (4): C3(\n","          (cv1): Conv(\n","            (conv): Conv2d(320, 160, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(320, 160, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv3): Conv(\n","            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (m): Sequential(\n","            (0): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (1): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (2): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (3): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (4): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (5): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (6): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (7): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","          )\n","        )\n","        (5): Conv(\n","          (conv): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (6): C3(\n","          (cv1): Conv(\n","            (conv): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv3): Conv(\n","            (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (m): Sequential(\n","            (0): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (1): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (2): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (3): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (4): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (5): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (6): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (7): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (8): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (9): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (10): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (11): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","          )\n","        )\n","        (7): Conv(\n","          (conv): Conv2d(640, 1280, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (8): C3(\n","          (cv1): Conv(\n","            (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv3): Conv(\n","            (conv): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (m): Sequential(\n","            (0): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (1): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (2): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (3): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","          )\n","        )\n","        (9): SPPF(\n","          (cv1): Conv(\n","            (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(2560, 1280, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (m): MaxPool2d(kernel_size=5, stride=1, padding=2, dilation=1, ceil_mode=False)\n","        )\n","        (10): Conv(\n","          (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (11): Upsample(scale_factor=2.0, mode=nearest)\n","        (12): Concat()\n","        (13): C3(\n","          (cv1): Conv(\n","            (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv3): Conv(\n","            (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (m): Sequential(\n","            (0): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (1): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (2): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (3): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","          )\n","        )\n","        (14): Conv(\n","          (conv): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (15): Upsample(scale_factor=2.0, mode=nearest)\n","        (16): Concat()\n","        (17): C3(\n","          (cv1): Conv(\n","            (conv): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(640, 160, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv3): Conv(\n","            (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (m): Sequential(\n","            (0): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (1): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (2): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (3): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","          )\n","        )\n","        (18): Conv(\n","          (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (19): Concat()\n","        (20): C3(\n","          (cv1): Conv(\n","            (conv): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(640, 320, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv3): Conv(\n","            (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (m): Sequential(\n","            (0): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (1): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (2): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (3): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","          )\n","        )\n","        (21): Conv(\n","          (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","          (act): SiLU(inplace=True)\n","        )\n","        (22): Concat()\n","        (23): C3(\n","          (cv1): Conv(\n","            (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv2): Conv(\n","            (conv): Conv2d(1280, 640, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (cv3): Conv(\n","            (conv): Conv2d(1280, 1280, kernel_size=(1, 1), stride=(1, 1))\n","            (act): SiLU(inplace=True)\n","          )\n","          (m): Sequential(\n","            (0): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (1): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (2): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","            (3): Bottleneck(\n","              (cv1): Conv(\n","                (conv): Conv2d(640, 640, kernel_size=(1, 1), stride=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","              (cv2): Conv(\n","                (conv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","                (act): SiLU(inplace=True)\n","              )\n","            )\n","          )\n","        )\n","        (24): Detect(\n","          (m): ModuleList(\n","            (0): Conv2d(320, 255, kernel_size=(1, 1), stride=(1, 1))\n","            (1): Conv2d(640, 255, kernel_size=(1, 1), stride=(1, 1))\n","            (2): Conv2d(1280, 255, kernel_size=(1, 1), stride=(1, 1))\n","          )\n","        )\n","      )\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":6}],"source":["test_df = pd.read_csv(os.path.join(main_dir, 'sample_submission.csv'))\n","yolo_v5 = torch.hub.load('ultralytics/yolov5', 'yolov5x', pretrained=True).cuda()\n","yolo_v5.eval()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"59ZoHi3U3Zqb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1677482699765,"user_tz":-540,"elapsed":690733,"user":{"displayName":"Claire Lim","userId":"01177983436534277283"}},"outputId":"720aba23-9a34-4db4-b486-59072aa2bc84"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1600/1600 [11:30<00:00,  2.32it/s]\n"]}],"source":["from PIL import Image\n","import torch\n","from torchvision import transforms\n","convert_tensor = transforms.ToTensor()\n","\n","test_data = {'path': [], 'x1': [], 'y1': [], 'x2': [], 'y2': []}\n","\n","total_test_imgs = []\n","for i in range(len(test_df)):\n","  total_test_imgs.append(os.path.join(test_img_path, test_df.iloc[i, 0]))\n","\n","w, h = 900, 900\n","offset = np.array([w//2, h//2])\n","\n","for idx, path in tqdm(enumerate(total_test_imgs), total=len(total_test_imgs)):\n","  w, h = 900, 900\n","  offset = np.array([w//2, h//2])\n","\n","  img = cv2.imread(path)[:, :, ::-1]\n","  centre = np.array(img.shape[:-1])//2\n","  x1,y1 = centre - offset\n","  x2,y2 = centre + offset\n","\n","  with torch.no_grad():\n","    cropped_img = img[x1:x2, y1:y2, :]\n","    results = yolo_v5([cropped_img])\n","\n","  cropped_img = cv2.cvtColor(cropped_img, cv2.COLOR_BGR2RGB)\n","  try:\n","    for i in range(len(results.xyxy[0])):\n","      xyxy = results.xyxy[0][i].detach().cpu().numpy()\n","      cropped_centre = np.array([(xyxy[0]+xyxy[2])/2, (xyxy[1]+xyxy[3])/2], dtype=np.float32)\n","      box_w = (xyxy[2]-xyxy[0])/2 * 1.2\n","      box_h = (xyxy[3]-xyxy[1])/2 * 1.2\n","\n","      new_x1 = np.clip(int(cropped_centre[0] - box_w), 0, img.shape[1])\n","      new_x2 = np.clip(int(cropped_centre[0] + box_w), 0, img.shape[1])\n","      new_y1 = np.clip(int(cropped_centre[1] - box_h), 0, img.shape[0])\n","      new_y2 = np.clip(int(cropped_centre[1] + box_h), 0, img.shape[0])\n","\n","      if int(xyxy[-1]) == 0:\n","        new_x1 += y1\n","        new_x2 += y1\n","        new_y1 += x1\n","        new_y2 += x1\n","\n","        test_data['path'].append(path)\n","        test_data['x1'].append(new_x1)\n","        test_data['y1'].append(new_y1)\n","        test_data['x2'].append(new_x2)\n","        test_data['y2'].append(new_y2)\n","        \n","  except Exception as e:\n","    print(\"Skip\")\n","\n","\n","test_df = pd.DataFrame(data=test_data)\n","test_df.to_csv(os.path.join(main_dir, 'test_bbox.csv'), index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"negGMpHV3Zqd"},"outputs":[],"source":["from typing import List\n","\n","class SingleModelTestConfig:\n","  def __init__(self,\n","               input_size: List[int] = [512, 512],\n","               num_joints: int = 24,\n","               kpd: float = 4.0,\n","               main_dir: str = main_dir,\n","               target_type: str = \"gaussian\",\n","               post_processing: str = \"dark\",\n","    ):\n","\n","    self.main_dir = main_dir\n","    self.image_size = np.array(input_size)\n","    self.num_joints = num_joints\n","    self.kpd = kpd\n","    self.target_type = target_type\n","    self.post_processing = post_processing\n","\n","    self.joints_name = {\n","          0: 'nose', 1: 'left_eye', 2: 'right_eye', 3: 'left_ear', 4: 'right_ear',\n","          5: 'left_shoulder', 6: 'right_shoulder', 7: 'left_elbow', 8: 'right_elbow',\n","          9: 'left_wrist', 10: 'right_wrist', 11: 'left_hip', 12: 'right_hip',\n","          13: 'left_knee', 14: 'right_knee', 15: 'left_ankle', 16: 'right_ankle',\n","          17: 'neck', 18: 'left_palm', 19: 'right_palm', 20: 'back_spine', 21: 'waist_spine',\n","          22: 'left_instep', 23: 'right_instep'\n","    }\n","\n","    self.joint_pair = [\n","          (0, 1), (0, 2), (2, 4), (1, 3), (6, 8), (8, 10),\n","          (5, 7), (7, 9), (11, 13), (13, 15), (12, 14), \n","          (14, 16), (5, 6), (15, 22), (16, 23), (11, 21),\n","          (21, 12), (20, 21), (5, 20), (6, 20), (17, 6), (17, 5)\n","    ]\n","\n","    self.flip_pair = [\n","          (1, 2), (3, 4), (5, 6), (7, 8),\n","          (9, 10), (11, 12), (13, 14), (15, 16),\n","          (18, 19), (22, 23)\n","    ]\n","\n","    cmap = plt.get_cmap(\"rainbow\")\n","    colors = [cmap(i) for i in np.linspace(0, 1,  self.num_joints + 2)]\n","    colors = [(c[2] * 255, c[1] * 255, c[0] * 255) for c in colors]\n","    self.joint_colors = {k: colors[k] for k in range(self.num_joints)}"]},{"cell_type":"markdown","metadata":{"id":"8XJXDmllLnS9"},"source":["- bbox의 중점을 기준으로\n","  - 앉아 있는 경우\n","  - 누워 있는 경우\n","  - 서 있는 경우\n","  - 위 세가지 경우로 나누어서 잘라낼 영역의 중점을 계산하고 affine transformation 해줬음"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PqLfWZZE3Zqe"},"outputs":[],"source":["class DaconKeypointsBBoxTestDataset(Dataset):\n","    def __init__(\n","        self, image_size,\n","        submission_df, transforms=None,\n","    ) -> None:\n","        self.df = submission_df\n","        self.image_size = image_size\n","        self.transforms = transforms\n","\n","    def __len__(self) -> int:\n","        return self.df.shape[0]\n","    \n","    def __getitem__(self, index: int):\n","        image_path = self.df.iloc[index, 0]\n","\n","        img_name = image_path.split('/')[-1]\n","        if img_name[:3] in [\"649\", \"650\", \"665\", \"666\"]:\n","          offset_h = 380\n","          offset_w = int(offset_h*1.333)\n","        elif img_name[:3] in [\"785\", \"786\"]:\n","          offset_h = 220\n","          offset_w = int(offset_h*1.333)\n","        else:\n","          offset_w = 300\n","          offset_h = int(offset_w*1.333)\n","\n","        image = cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n","        image_centre = np.array(image.shape[:-1])//2\n","\n","        x1, y1, x2, y2 = self.df.iloc[index, 1:]\n","        bbox_centre = np.array([\n","                          (x1+x2)//2,\n","                          (y1+y2)//2\n","                      ])\n","        \n","        cropped_y2 = np.clip(bbox_centre[1]+offset_h, 0, image.shape[0])\n","        cropped_y1 = np.clip(bbox_centre[1]-offset_h, 0, image.shape[0])\n","        cropped_x2 = np.clip(bbox_centre[0]+offset_w, 0, image.shape[1])\n","        cropped_x1 = np.clip(bbox_centre[0]-offset_w, 0, image.shape[1])\n","        \n","        x, y, w, h = cropped_x1, cropped_y1, cropped_x2-cropped_x1, cropped_y2-cropped_y1\n","        aspect_ratio = self.image_size[1] / self.image_size[0]\n","        centre = np.array([x+w*.5, y+h*.5])\n","        if w > aspect_ratio * h:\n","            h = w * 1.0 / aspect_ratio\n","        elif w < aspect_ratio * h:\n","            w = h * aspect_ratio\n","\n","        image_centre = np.array([cropped_y1, cropped_x1])\n","        scale = np.array([w, h])\n","        rotation = 0\n","\n","        trans = get_affine_transform(centre, scale, rotation, (self.image_size[1], self.image_size[0]))\n","        cropped_img = cv2.warpAffine(image, trans, (self.image_size[1], self.image_size[0]), flags=cv2.INTER_LINEAR)\n","        cropped_img_shape = np.array([h, w])\n","\n","\n","        if self.transforms:\n","          transposed_img = self.transforms(image=cropped_img)['image']\n","\n","        \n","        sample = {\n","                  'transposed_img': torch.from_numpy(transposed_img).float().permute(2, 0, 1),\n","                  'centre': torch.from_numpy(centre).float(),\n","                  'scale': torch.from_numpy(scale).float()\n","                 }\n","        return sample"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OEyRb4L03ZrQ"},"outputs":[],"source":["def flip_back(output_flipped, matched_parts):\n","    '''\n","    ouput_flipped: numpy.ndarray(batch_size, num_joints, height, width)\n","    '''\n","    assert output_flipped.ndim == 4,\\\n","        'output_flipped should be [batch_size, num_joints, height, width]'\n","\n","    output_flipped = output_flipped[:, :, :, ::-1]\n","\n","    for pair in matched_parts:\n","        tmp = output_flipped[:, pair[0], :, :].copy()\n","        output_flipped[:, pair[0], :, :] = output_flipped[:, pair[1], :, :]\n","        output_flipped[:, pair[1], :, :] = tmp\n","\n","    return output_flipped"]},{"cell_type":"markdown","metadata":{"id":"o2zWtL7_MZXe"},"source":["- 예측 함수 새로 정의"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9S4NRzCd3ZrQ"},"outputs":[],"source":["def transform_preds(coords, center, scale, output_size):\n","    target_coords = np.zeros(coords.shape)\n","    trans = get_affine_transform(center, scale, 0, output_size, inv=1)\n","    for p in range(coords.shape[0]):\n","        target_coords[p, 0:2] = affine_transform(coords[p, 0:2], trans)\n","    return target_coords\n","\n","\n","def get_final_preds2(cfg, batch_heatmaps, center, scale):\n","    heatmap_height = batch_heatmaps.shape[2]\n","    heatmap_width = batch_heatmaps.shape[3]\n","    if cfg.target_type == 'gaussian':\n","        coords, maxvals = get_max_preds(batch_heatmaps)\n","        if cfg.post_processing == \"dark\":\n","            coords = dark_post_processing(coords,batch_heatmaps)\n","\n","    preds = coords.copy()\n","    preds_in_input_space = preds.copy()\n","    preds_in_input_space[:,:, 0] = preds_in_input_space[:,:, 0] / (heatmap_width - 1.0) * (4 * heatmap_width - 1.0)\n","    preds_in_input_space[:,:, 1] = preds_in_input_space[:,:, 1] / (heatmap_height - 1.0) * (4 * heatmap_height - 1.0)\n","    # Transform back\n","    for i in range(coords.shape[0]):\n","        preds[i] = transform_preds(\n","            coords[i], center[i], scale[i], [heatmap_width, heatmap_height]\n","        )\n","\n","    return preds, preds_in_input_space"]},{"cell_type":"code","execution_count":24,"metadata":{"id":"klpsLPQv3ZrQ","executionInfo":{"status":"ok","timestamp":1677482930506,"user_tz":-540,"elapsed":1042,"user":{"displayName":"Claire Lim","userId":"01177983436534277283"}}},"outputs":[],"source":["def bbox_test(cfg, filp_test=False, debug=False):\n","  global main_dir\n","  flip_pair = [\n","          (1, 2), (3, 4), (5, 6), (7, 8),\n","          (9, 10), (11, 12), (13, 14), (15, 16),\n","          (18, 19), (22, 23)\n","    ]\n","\n","  seed_everything(2021)\n","  \n","  predictions = []\n","  test_tfms = A.Normalize()\n","  \n","  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","  model = PoseHighResolutionNet(48)\n","  model.final_layer = nn.Conv2d(48, 24, 1)\n","  bestmodel_path = '/content/drive/MyDrive/방학 CV분반 KUBIG CONTEST/임채명/kubigcontestdata/result/best_model.pth'\n","  model.load_state_dict(torch.load(bestmodel_path))\n","\n","  model = model.to(device)\n","\n","  submission_path = os.path.join(main_dir, 'test_bbox.csv')\n","  submission = pd.read_csv(submission_path)\n","  test_ds = DaconKeypointsBBoxTestDataset(cfg.image_size, submission, test_tfms)\n","  test_dl = DataLoader(test_ds, batch_size=32, shuffle=False)\n","  \n","  img_num = 1\n","  save_folder = os.path.join(main_dir, \"debug/test_data\")\n","  if not os.path.exists(save_folder): os.makedirs(save_folder, exist_ok=True)\n","\n","  model.eval()\n","  with tqdm(test_dl, total=test_dl.__len__(), unit=\"batch\") as test_bar:\n","        for sample in test_bar:\n","            images = sample['transposed_img'].to(device)\n","            scale = sample['scale'].detach().cpu().numpy()\n","            center = sample['centre'].detach().cpu().numpy()\n","\n","            with torch.no_grad():\n","              preds = model(images)\n","              if filp_test:\n","                inp_flip = images.clone().flip(3)\n","                flip_preds = model(inp_flip)\n","                flip_preds = flip_back(flip_preds.cpu().numpy(), flip_pair)\n","                flip_preds = torch.from_numpy(flip_preds.copy()).to(device)\n","                preds = (preds + flip_preds)*0.5\n","\n","              heatmap_height = preds.shape[2]\n","              heatmap_width = preds.shape[3]\n","              pred_coords, pred_coords_input_space = get_final_preds2(cfg, preds.detach().cpu().numpy(), center, scale)\n","              pred_coords = pred_coords.astype(np.float32)\n","              predictions.extend(pred_coords)\n","  return np.array(predictions)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":252318,"status":"ok","timestamp":1677483185073,"user":{"displayName":"Claire Lim","userId":"01177983436534277283"},"user_tz":-540},"id":"quzh5pup3ZrQ","outputId":"3f925282-1b2d-4e1e-e514-ec36188875ea"},"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 0/50 [00:00<?, ?batch/s]<ipython-input-13-c0da3f4d6d85>:111: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  coords = coords.astype(np.float)\n","100%|██████████| 50/50 [04:08<00:00,  4.97s/batch]\n"]}],"source":["cfg = SingleModelTestConfig(input_size=[512, 512], target_type='gaussian')\n","predictions = bbox_test(cfg, filp_test=True, debug=False)"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"3_nZWHqP3ZrR","executionInfo":{"status":"ok","timestamp":1677483185074,"user_tz":-540,"elapsed":9,"user":{"displayName":"Claire Lim","userId":"01177983436534277283"}}},"outputs":[],"source":["preds = []\n","for prediction in predictions:\n","  row = []\n","  \n","  for x,y in zip(prediction[:, 0], prediction[:, 1]):\n","    row.append(x)\n","    row.append(y)\n","  preds.append(row)\n","preds = np.array(preds)"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"btfudVQD3ZrR","executionInfo":{"status":"ok","timestamp":1677483185075,"user_tz":-540,"elapsed":7,"user":{"displayName":"Claire Lim","userId":"01177983436534277283"}}},"outputs":[],"source":["submission_path = os.path.join(main_dir, 'sample_submission.csv')\n","submission = pd.read_csv(submission_path)"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"OUw847oC3ZrR","executionInfo":{"status":"ok","timestamp":1677483185075,"user_tz":-540,"elapsed":7,"user":{"displayName":"Claire Lim","userId":"01177983436534277283"}}},"outputs":[],"source":["save_path = os.path.join(main_dir, 'final_submissions.csv')\n","submission.iloc[:, 1:] = preds\n","submission.to_csv(save_path, index=False)"]},{"cell_type":"markdown","metadata":{"id":"WXjFn_ojQRPZ"},"source":["해볼만한 시도\n","- 전처리, 후처리 다르게\n","- 테스트시 사용하는 detection 모델 다르게 (YOLOv5 말고 다른 걸로)\n","\n","\n","\n","\n","1. epoch 바꿔보기 -> epoch 5, 15, 30 중에 15가 가장 높은 결과\n","2. loss를 cross entropy로 바꿔보기 -> 성능 오히려 떨어짐..\n","3. learning rate scheduler 사용해보기\n","4. input size 512x512 (1등 코드 참고)"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1CGETHvdQP8onaxKp9ArY91MaNv-xndyM","timestamp":1676778171205}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4719ccc2631c497a8054cab6dd472bef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6b64c35642f4e4bbd4c04b5752ccf5c","IPY_MODEL_df0230fc30544e8e84f8a28f3463ed73","IPY_MODEL_25293b5e793e4b61a96d173e2360134c"],"layout":"IPY_MODEL_24d568998cdb45d895de442f214a8ba4"}},"e6b64c35642f4e4bbd4c04b5752ccf5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8a4a25ea0510462b85c12664880e49c2","placeholder":"​","style":"IPY_MODEL_52b747683eb64e909261488ee8ecae70","value":"100%"}},"df0230fc30544e8e84f8a28f3463ed73":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d7841719e304462b739991fd7550b30","max":174114333,"min":0,"orientation":"horizontal","style":"IPY_MODEL_be336f286abd481e84f9c01ff1cde7a0","value":174114333}},"25293b5e793e4b61a96d173e2360134c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_99c1d651409b46419a3633fdd3fc3cda","placeholder":"​","style":"IPY_MODEL_e59d8372c4e34f308278fed88b3b2a3f","value":" 166M/166M [00:02&lt;00:00, 65.1MB/s]"}},"24d568998cdb45d895de442f214a8ba4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a4a25ea0510462b85c12664880e49c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"52b747683eb64e909261488ee8ecae70":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d7841719e304462b739991fd7550b30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be336f286abd481e84f9c01ff1cde7a0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"99c1d651409b46419a3633fdd3fc3cda":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e59d8372c4e34f308278fed88b3b2a3f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}